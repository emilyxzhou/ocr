import cv2
import math
import numpy as np

from tools import display_image, Constants


class OCREngineBase:

    def __init__(self):
        self._model_type = None
        self._model = None

    def segment_characters(self, image, is_generated_text=False):
        """
        Calls the appropriate character segmentation method according to the is_generated_text flag.
        Generated text refers to images with blocks of text overlaid with data_generator.py.
        """
        if is_generated_text:
            return self.segment_characters_from_generated_text(image)
        else:
            return self.segment_characters_from_camera(image)

    def segment_characters_from_generated_text(self, image):
        """
        Input image is digitally generated by overlaying characters onto a background image.
        Segments the input image into individual characters.
        Returns a list of cropped images containing a single character.
        """
        if type(image) is not np.ndarray:
            raise TypeError(f"Input image is not a numpy array.")
        image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        _, th = cv2.threshold(image_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        # temp = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        # th_dilated = cv2.morphologyEx(th, cv2.MORPH_DILATE, temp)
        cv2.imshow("edges", th)
        cv2.waitKey(0)
        contours, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        rectangles = [cv2.boundingRect(c) for c in contours]
        sorted_bounding_boxes = self._sort_bounding_boxes(rectangles)
        lines = self.get_lines_from_bounding_boxes(image_gray, sorted_bounding_boxes)
        # cv2.imshow("boxes", image)
        # cv2.waitKey(0)
        return lines

    def segment_characters_from_camera(self, image):
        """
        Input image is taken with a webcam.
        Segments the input image into individual characters.
        Returns a list of cropped images containing a single character.
        """
        new_bw = self._get_bw_image(image)
        sorted_bounding_boxes = self.filter_characters(new_bw)
        lines = self.get_lines_from_bounding_boxes(image, sorted_bounding_boxes)
        return lines

    def filter_characters(self, image):
        """Returns all rectangles found in image."""
        # Filter out invalid rectangles
        contours, hierarchy = cv2.findContours(image.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        rectangles = []
        average_area = np.mean([cv2.contourArea(c) for c in contours])
        min_area = np.min([cv2.contourArea(c) for c in contours])
        max_area = np.max([cv2.contourArea(c) for c in contours])
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            contour_area = cv2.contourArea(contour)
            is_rectangle_valid = contour_area > average_area * 0.6
            if is_rectangle_valid:
                rectangles.append((x, y, w, h))
        # rectangles = remove_contained_rectangles(rectangles)
        rectangles = self.remove_outlier_rectangles(rectangles)
        rectangles = self._sort_bounding_boxes(rectangles)
        return rectangles

    def _get_bw_image(self, image):
        """
        Given an image, identifies text and segments the image into foreground and background such that text is white
        and the rest of the image is black.
        :param image: Image taken from a camera.
        :return: Input image with text in white and the rest in black.
        """
        # Convert image to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        # display_image(gray, "initial grayscale")
        _, gray = cv2.threshold(gray, 230, 255, cv2.THRESH_TOZERO)
        # display_image(gray, "zero thresholding")

        _, bw_copy = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
        # display_image(bw_copy, "bw copy")

        # Bilateral filtering
        blur = cv2.bilateralFilter(gray, 5, 75, 75)
        # display_image(blur, "blur")

        # Morphological gradient calculation
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        grad = cv2.morphologyEx(blur, cv2.MORPH_GRADIENT, kernel)
        # display_image(grad, "gradient")

        # Binarization (Otsu considers two classes of pixels)
        _, bw = cv2.threshold(grad, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
        # display_image(bw, "otsu")

        # Closing operation
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 1))
        closed = cv2.morphologyEx(bw, cv2.MORPH_CLOSE, kernel)
        # display_image(closed, "closed")

        # Finding contours
        contours, _ = cv2.findContours(closed, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

        mask = np.zeros(closed.shape, dtype=np.uint8)
        mask1 = np.zeros(bw_copy.shape, dtype=np.uint8)
        wb_copy = cv2.bitwise_not(bw_copy)
        new_bw = np.zeros(bw_copy.shape, dtype=np.uint8)

        print(f"{len(contours)} contours found")
        for i in range(len(contours)):
            x, y, w, h = cv2.boundingRect(contours[i])
            mask[y:y + h, x:x + w] = 0
            area = cv2.contourArea(contours[i])
            aspect_ratio = float(w) / h
            cv2.drawContours(mask, contours, i, (255, 255, 255), -1)
            # Percentage of nonzero pixels in the rectangular region
            r = float(cv2.countNonZero(mask[y:y + h, x:x + w])) / (w * h)

            # Identify region of interest
            if r > 0.2 and 0.2 < aspect_ratio < 8 and area > 150:

                cv2.drawContours(mask1, [contours[i]], -1, (255, 255, 255), -1)

                bw_temp = cv2.bitwise_and(mask1[y:y + h, x:x + w], bw_copy[y:y + h, x:x + w])
                wb_temp = cv2.bitwise_and(mask1[y:y + h, x:x + w], wb_copy[y:y + h, x:x + w])

                bw_count = cv2.countNonZero(bw_temp)
                wb_count = cv2.countNonZero(wb_temp)

                if bw_count > wb_count:
                    new_bw[y:y + h, x:x + w] = np.copy(bw_copy[y:y + h, x:x + w])
                else:
                    new_bw[y:y + h, x:x + w] = np.copy(wb_copy[y:y + h, x:x + w])

        # display_image(new_bw, "extracted text")
        return new_bw

    def remove_contained_rectangles(self, image, rectangles):
        valid_rectangles = []
        i = 0
        while i < len(rectangles):
            j = i + 1
            while j < len(rectangles):
                mask1 = np.zeros(image.shape, dtype=np.uint8)
                mask2 = np.zeros(image.shape, dtype=np.uint8)
                x1, y1, w1, h1 = rectangles[i]
                x2, y2, w2, h2 = rectangles[j]
                mask1[y1:y1 + h1, x1:x1 + w1] = 255
                mask2[y2:y2 + h2, x2:x2 + w2] = 255
                intersection = mask1 & mask2
                # if np.sum(intersection) > 0:
                #     display_image(mask1, "mask1")
                #     display_image(mask2, "mask2")
                #     display_image(intersection, "intersection")
                if np.sum(intersection) > 0:
                    if np.sum(intersection) == np.sum(mask2):  # rect2 (j) is fully contained within rect1 (i)
                        valid_rectangles.append((x1, y1, w1, h1))
                        rectangles.pop(j)
                        j -= 1
                    elif np.sum(intersection) == np.sum(mask1):  # rect1 (i) is fully contained within rect2 (j)
                        valid_rectangles.append((x2, y2, w2, h2))
                        rectangles.pop(i)
                        i -= 1
                        continue
                j += 1
            i += 1
        return valid_rectangles

    def remove_outlier_rectangles(self, rectangles):
        valid_rectangles = []
        x_values = np.asarray([r[0] for r in rectangles])
        y_values = np.asarray([r[1] for r in rectangles])
        x_dist = np.abs(x_values - np.median(x_values))
        y_dist = np.abs(y_values - np.median(y_values))
        med_x_dev = np.median(x_dist)
        med_y_dev = np.median(y_dist)

        for i in range(len(rectangles)):
            if x_dist[i] < med_x_dev * 2.5 and y_dist[i] < med_y_dev * 2.5:
                valid_rectangles.append(rectangles[i])

        return valid_rectangles

    def get_lines_from_bounding_boxes(self, image, bounding_boxes):
        lines = []
        for line in bounding_boxes:
            chars = []
            i = 0
            while i < len(line):
                box1 = line[i]
                (x, y, w, h) = box1
                if 0 < i < len(line) - 1:
                    box0 = line[i-1]
                    box1 = line[i]
                    box2 = line[i+1]
                    if self._is_skip_box(box0, box1, box2):
                        i += 1
                        continue

                # Compare current box to next box and see if they should be merged into one
                if i < len(line) - 1:
                    box2 = line[i+1]
                    if self._is_merge_boxes(box1, box2):
                        (x, y, w, h) = self._merge_boxes(box1, box2)
                        i += 1
                curr_num = image[y:y + h, x:x + w]
                chars.append(curr_num)
                # The following three lines are used for debugging purposes
                # cv2.rectangle(image, (x, y), (x + w, y + h), color=(0, 255, 0), thickness=1)
                # cv2.imshow("boxes", image)
                # cv2.waitKey(0)
                i += 1
            lines.append(chars)
        return lines

    def _sort_bounding_boxes(self, rectangles):
        """
        Sorts bounding boxes of contours top to bottom, left to right.
        Returns a list of lists, where the outer lists represent lines in order from top to bottom.
        Bounding boxes within each list are ordered from left to right.
        """
        valid_contour_boxes = []
        sorted_y = sorted(rectangles, key=lambda b: b[1] + b[3])
        new_line_indices = [0]
        min_y = sorted_y[0][1]
        max_char_height = max([bounds[3] for bounds in sorted_y])
        for box in sorted_y:
            if box[3] >= max_char_height / 3:
                valid_contour_boxes.append(box)
        # Separate bounding boxes into horizontal lines
        for i in range(len(valid_contour_boxes)):
            if valid_contour_boxes[i][1] > min_y + max_char_height:
                min_y = valid_contour_boxes[i][1]
                new_line_indices.append(i)
        # Sort boxes within lines from left to right
        lines = [valid_contour_boxes[
                 new_line_indices[i]:new_line_indices[i+1]
                 ] for i in range(len(new_line_indices)-1)]
        lines.append(sorted_y[new_line_indices[-1]:])
        sorted_lines = [sorted(line, key=lambda b: b[0] + b[2]) for line in lines]
        return sorted_lines

    def _is_skip_box(self, box0, box1, box2, threshold=15):
        """
        :param box0: Leftmost box of three adjacent boxes in the same line.
        :param box1: Middle box of three adjacent boxes in the same line.
        :param box2: Rightmost box of three adjacent boxes in the same line.
        :param threshold: Maximum number of pixels the middle box can differ from the left and right boxes
        :return: True if the y-coordinate of the upper left point of the middle box (box1) differs from the
            left and right boxes by more than the threshold number of pixels.
        """
        return abs(box1[1] - box0[1]) > threshold and abs(box2[1] - box1[1]) > threshold

    def _is_merge_boxes(self, box1, box2, x_thresh=1, y_thresh=3):
        """
        Checks if two contour boxes should be merged into one.
        :param box1: Contour box in the format (x, y, w, h). (x, y) denotes the top left corner.
        :param box2: Contour box in the format (x, y, w, h).
        :return: True if boxes have very close x or y coordinates and are nearly overlapping.
            'Close' if x or y coordinates are within the threshold # pixels of one another.
            'Nearly overlapping' if edges are within the threshold # pixels of one another.
        """
        (x1, y1, w1, h1) = box1
        (x2, y2, w2, h2) = box2
        if abs(x1 - x2) < x_thresh and (abs(y1 + h1 - y2) < y_thresh or abs(y2 + h2 - y1) < y_thresh):
            return True
        elif abs(y1 - y2) < y_thresh and (abs(x1 + w1 - x2) < x_thresh or abs(x2 + w2 - x1) < x_thresh):
            return True
        else:
            return False

    def _merge_boxes(self, box1, box2, x_thresh=1, y_thresh=3):
        """
        Merges two bounding boxes into one.
        :param box1: Contour box in the format (x, y, w, h). (x, y) denotes the top left corner.
        :param box2: Contour box in the format (x, y, w, h).
        :return: Merged bounding box in the format (x, y, w, h).
        """
        (x1, y1, w1, h1) = box1
        (x2, y2, w2, h2) = box2
        # Merge boxes vertically
        if abs(x1 - x2) < x_thresh:
            (x, y, w, h) = (
                max(x1, x2),
                min(y1, y2),
                max(w1, w2),
                h1 + h2
            )
        # Merge boxes horizontally
        elif abs(y1 - y2) < y_thresh:
            (x, y, w, h) = (
                min(x1, x2),
                min(y1, y2),
                w1 + w2,
                max(h1, h2)
            )
        else:
            raise ValueError("Boxes did not meet criteria for merging")

        return x, y, w, h

    def _resize_model_input(self, char):
        """
        Resizes the input character image to the input size of the OCR model.
        :param char: Image to be resized; numpy array expected
        :return: Resized image. Scales original image down such that the maximum height or width is
            equivalent to the model input side length. Pads remaining space with black pixels.
        """
        # The following 5 lines are for testing purposes.
        # print(f"Shape before resizing: {char.shape}")
        # cv2.namedWindow("char", cv2.WINDOW_NORMAL)
        # cv2.resizeWindow("char", 200, 200)
        # cv2.imshow("char", char)
        # cv2.waitKey(0)

        scale = min(Constants.IMAGE_SIZE / max(char.shape), 1)
        char = cv2.resize(char, None, fx=scale, fy=scale)
        l_r = (math.floor((Constants.IMAGE_SIZE - char.shape[1])/2), (math.ceil((Constants.IMAGE_SIZE - char.shape[1])/2)))
        t_b = (math.floor((Constants.IMAGE_SIZE - char.shape[0])/2), (math.ceil((Constants.IMAGE_SIZE - char.shape[0])/2)))
        char = np.pad(char, (t_b, l_r, (0, 0)), mode="constant", constant_values=0)
        char = cv2.cvtColor(char, cv2.COLOR_BGR2GRAY)
        # print(f"Shape after resizing: {char.shape}")
        # cv2.imshow("resized", char)
        # cv2.waitKey(0)

        return char
